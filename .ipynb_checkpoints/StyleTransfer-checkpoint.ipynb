{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab3973d8-7f0f-4f59-9979-315af0296d3e",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132a8d6d-9548-486b-ab2a-a65a231bd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import PIL.Image as Image\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa755bcb-5af0-4b85-af18-306b5d45ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    img = PIL.Image.open(path)\n",
    "    img = img.resize((32, 32))\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    return tf.convert_to_tensor(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75fccb4-4d74-46c9-9337-13f275109376",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = get_img('input_img/dancing.jpg')\n",
    "content_img = tf.reshape(content_img, [1,32,32,3])\n",
    "style_img = get_img('input_img/saryan.jpg')\n",
    "style_img = tf.reshape(style_img, [1,32,32,3])\n",
    "target_img = 0.5*(content_img+style_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e567c3-57ed-4374-9761-afc52d6a4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.vgg16.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(32,32,3),\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "vgg_layers = []\n",
    "vgg_layers.append(base_model.layers[0])\n",
    "for layer in base_model.layers:\n",
    "    if \"conv\" in layer.name:\n",
    "        layer.trainable = False\n",
    "        vgg_layers.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44926a57-3024-449d-a363-24043fa8a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_Net(layers):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    for layer in layers[:6]:\n",
    "        layer.trainable = True\n",
    "        model.add(layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028e8d3-9ce9-4ac6-a8fa-34a0a9139afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG_Net(vgg_layers)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f614743-24f6-4850-a5d5-86d667a52b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_c = VGG_Net(vgg_layers)\n",
    "vgg_s = VGG_Net(vgg_layers)\n",
    "vgg_t = VGG_Net(vgg_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a7c07-eb00-422c-8172-a7644908315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "b_loss = keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19711a-94df-43ca-9c2f-1508f7eee55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_t.compile(loss=b_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2820a-df84-4c66-8e27-b6e66f87812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "target0 = vgg_t(target_img)\n",
    "\n",
    "loss_collection= []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    target = vgg_t(target_img)\n",
    "\n",
    "    loss, loss_c, loss_s = 0, 0, 0\n",
    "\n",
    "    cont_l = content_img\n",
    "    style_l = style_img\n",
    "    target_l = target_img\n",
    "\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        for n,layer in enumerate(vgg_t.layers):\n",
    "\n",
    "            cont_l = vgg_c.layers[n](cont_l)\n",
    "            target_l = vgg_t.layers[n](target_l)\n",
    "\n",
    "            loss_c += tf.norm(cont_l-target_l)\n",
    "            \n",
    "            style_l = vgg_s.layers[n](style_l)\n",
    "            _, c, h, w = style_l.shape\n",
    "            style_l_t = style_l.reshape(c, h * w)\n",
    "            target_l_t = target_l.reshape(c, h * w)\n",
    "            \n",
    "            loss_s += tf.norm(style_l_t-target_l_t)\n",
    "\n",
    "    loss = loss_c\n",
    "    grad = g_tape.gradient(loss, vgg_t.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grad, vgg_t.trainable_variables))\n",
    "    loss_collection.append(loss)\n",
    "\n",
    "target1 = vgg_t(target_img)\n",
    "print(tf.norm(target1-target0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd12617-2172-47e5-bcb0-150ed37eaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_collection)\n",
    "plt.ylabel(\"Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gig",
   "language": "python",
   "name": "gig"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
